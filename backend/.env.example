# Git Metrics Detector - backend/.env.example
#
# Copy this file to backend/.env and fill in the values you need.
#
# IMPORTANT:
# - Do NOT commit backend/.env or any service account JSON to git.
# - This project runs entirely in user-space (no admin required).

# ── LLM Provider ───────────────────────────────────────────────
# Which provider to prefer when multiple are configured:
#   ollama | gemini | groq | openrouter
LLM_PROVIDER=ollama

# ── Google Gemini (Vertex or AI Studio) ────────────────────────
# If you use a Gemini Vertex service account, set:
#   GEMINI_SERVICE_ACCOUNT_FILE=service-account.json
# and the backend will use Gemini for ALL LLM calls (no fallback mixing).
GEMINI_SERVICE_ACCOUNT_FILE=

# Optional: model override
GEMINI_MODEL=gemini-2.0-flash

# Gemini AI Studio API key (optional if using service account)
GEMINI_API_KEY=

# ── Groq (optional) ────────────────────────────────────────────
GROQ_API_KEY=

# ── OpenRouter (optional) ──────────────────────────────────────
OPENROUTER_API_KEY=

# ── Ollama (local, no API key needed) ──────────────────────────
OLLAMA_BASE_URL=http://localhost:11434

# ── GitHub (optional, recommended) ─────────────────────────────
GITHUB_TOKEN=

# ── Database ───────────────────────────────────────────────────
DATABASE_URL=sqlite+aiosqlite:///./data/metrics.db

# ── LLM Prompt Limits ──────────────────────────────────────────
# Large files can cause timeouts/empty LLM responses. The backend truncates
# file contents to this many characters per file in prompts.
LLM_MAX_FILE_CHARS=6000

# ── Metabase (used to build dashboards) ─────────────────────────
# Run Metabase locally (default port 3003). First run requires browser setup.
METABASE_URL=http://localhost:3003
METABASE_USERNAME=
METABASE_PASSWORD=
